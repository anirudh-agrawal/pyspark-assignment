{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession, SQLContext, Row\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"WeatherAnalysis\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-10-0-1-11.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>WeatherAnalysis</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f3e60d3f990>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading WeatherStations Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherStationsSchema = T.StructType([\n",
    "    T.StructField(\"State\",T.StringType()),\n",
    "    T.StructField(\"District\",T.StringType()),\n",
    "    T.StructField(\"Latitude\",T.DoubleType()),\n",
    "    T.StructField(\"Longitude\",T.DoubleType()),\n",
    "    T.StructField(\"StationID\",T.StringType()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherStations = spark.read.load(path = \"Datasets/Output/WeatherStationsId.csv\",\n",
    "                                  format = \"csv\",header=True,\n",
    "                                  schema = weatherStationsSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- State: string (nullable = true)\n",
      " |-- District: string (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- Longitude: double (nullable = true)\n",
      " |-- StationID: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weatherStations.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+------------------+---------+---------+\n",
      "|          State|      District|          Latitude|Longitude|StationID|\n",
      "+---------------+--------------+------------------+---------+---------+\n",
      "|      TELANGANA|      NALGONDA|  16.3920001983643|   79.375|   164794|\n",
      "|          BIHAR|CHAMPARAN.WEST|  27.3199996948242|  83.4375|   273834|\n",
      "|          BIHAR|   MUZAFFARPUR|  25.4466991424561|  85.3125|   254853|\n",
      "|          BIHAR|    SAMASTIPUR|  25.4466991424561|  85.3125|   254853|\n",
      "|        GUJARAT|     AHMEDABAD|  22.9487991333008|     72.5|   229725|\n",
      "|      KARNATAKA|     BAGALKOTE|  16.0797996520996|   75.625|   161756|\n",
      "|      KARNATAKA|BANGALORE URBA|12.957500457763699|     77.5|   130775|\n",
      "|         KERALA|        IDUKKI|10.147399902343802|  77.1875|   101772|\n",
      "|MADHAYA PRADESH|   NARSINGHPUR|  7.02514982223511|   79.375|    70794|\n",
      "|    MAHARASHTRA|    AHMEDNAGAR|   18.577600479126|  75.3125|   186753|\n",
      "|    MAHARASHTRA|       YEOTMAL|  20.1387996673584|  78.4375|   201784|\n",
      "|         ORISSA|       BHADRAK|  21.3876991271973|    86.25|   214863|\n",
      "|         PUNJAB|      LUDHIANA|  30.4423007965088|  75.3125|   304753|\n",
      "|      RAJASTHAN|        BARMER|  25.4466991424561|  72.1875|   254722|\n",
      "|      TAMILNADU|       VELLORE|   12.645299911499|  79.0625|   126791|\n",
      "|  UTTAR PRADESH|       MATHURA|  27.3199996948242|  77.1875|   273772|\n",
      "|  UTTAR PRADESH|     MORADABAD|  29.1933994293213|  78.4375|   292784|\n",
      "|  UTTAR PRADESH|     SHRAWASTI|  27.3199996948242|  81.5625|   273816|\n",
      "|    WEST BENGAL|       BANKURA|23.260999679565398|  87.1875|   233872|\n",
      "|    WEST BENGAL|    JALPAIGURI|    26.38330078125|   89.375|   264894|\n",
      "+---------------+--------------+------------------+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weatherStations.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of relevant data files to read\n",
    "~~~Text\n",
    "The weather information for each station is in a seperate file.<br>\n",
    "Generating list of data files to read.<br>\n",
    "\n",
    "Each filename is of the format:\n",
    "            weatherdata-<id>.csv\n",
    "where id is the unique identifier\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get list of unique StationID in the data\n",
    "stationIDs = weatherStations.select(\"StationID\").distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1340"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get count of unique StationID\n",
    "stationIDs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating List\n",
    "stationIDs = [row[\"StationID\"] for row in stationIDs.collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['151772', '220725', '139744', '192750', '245841']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stationIDs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating filenames \n",
    "filenames = [\"weatherdata-{ID}.csv\".format(ID=stationId) for stationId in stationIDs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['weatherdata-151772.csv',\n",
       " 'weatherdata-220725.csv',\n",
       " 'weatherdata-139744.csv',\n",
       " 'weatherdata-192750.csv',\n",
       " 'weatherdata-245841.csv']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading WeatherData files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funtion to format date to required format\n",
    "def formatDate(date):\n",
    "    month,day,year = date.split(\"/\")\n",
    "    \n",
    "    if len(day)==1: \n",
    "        day=\"0\"+day\n",
    "    \n",
    "    if len(month)==1: \n",
    "        month=\"0\"+month\n",
    "    \n",
    "    return \"-\".join([year,month,day])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating UDF\n",
    "formatDate=F.udf(formatDate,T.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing the UDF and transformations on one file\n",
    "weatherDataDF = spark.read.load(\n",
    "                    path = \"Datasets/2000_2013/files/\"+filenames[0],\n",
    "                    format=\"csv\", header=True)\n",
    "\n",
    "weatherDataDF = ( weatherDataDF.select(\"Date\",F.col(\"Max Temperature\").cast(T.DoubleType()),\n",
    "                                         F.col(\"Min Temperature\").cast(T.DoubleType()),\n",
    "                                         F.col(\"Precipitation\").cast(T.DoubleType()),\n",
    "                                         F.col(\"Relative Humidity\").cast(T.DoubleType()))\n",
    "                              .withColumn(\"StationId\",F.lit(stationIDs[0]))\n",
    "                              .withColumn(\"Date\",F.to_date(formatDate(F.col(\"Date\")),format=\"yyyy-MM-dd\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Max Temperature: double (nullable = true)\n",
      " |-- Min Temperature: double (nullable = true)\n",
      " |-- Precipitation: double (nullable = true)\n",
      " |-- Relative Humidity: double (nullable = true)\n",
      " |-- StationId: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#checcking schema\n",
    "weatherDataDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+-----------------+-------------------+---------+\n",
      "|summary|  Max Temperature|   Min Temperature|    Precipitation|  Relative Humidity|StationId|\n",
      "+-------+-----------------+------------------+-----------------+-------------------+---------+\n",
      "|  count|             5114|              5114|             5114|               5114|     5114|\n",
      "|   mean|33.54038815017596|21.566215095815412|1.947447703802815| 0.5326649542642927| 151772.0|\n",
      "| stddev|4.450241019026345|3.7775547416558575|6.534890647535881|0.18298700348943317|      0.0|\n",
      "|    min|           21.061|            10.409|              0.0| 0.0889887002959665|   151772|\n",
      "|    max|           46.742|            30.337|      142.3073088|  0.953042742704384|   151772|\n",
      "+-------+-----------------+------------------+-----------------+-------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weatherDataDF.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+---------------+-------------+-----------------+---------+\n",
      "|      Date|Max Temperature|Min Temperature|Precipitation|Relative Humidity|StationId|\n",
      "+----------+---------------+---------------+-------------+-----------------+---------+\n",
      "|2000-01-01|         28.874|         14.754|          0.0|0.529431046214477|   151772|\n",
      "|2000-01-02|         29.225|         14.758|          0.0| 0.57896803144991|   151772|\n",
      "|2000-01-03|         29.165|         14.417|          0.0|0.619197799378162|   151772|\n",
      "|2000-01-04|         29.341|         16.189|          0.0|0.604262783080777|   151772|\n",
      "|2000-01-05|         29.453|         16.164|          0.0|0.590774879511934|   151772|\n",
      "|2000-01-06|         30.316|         16.093|          0.0|0.553721838117377|   151772|\n",
      "|2000-01-07|         30.326|         14.239|          0.0|0.299192999999843|   151772|\n",
      "|2000-01-08|         29.333|         15.986|  0.291824604|0.606178057238691|   151772|\n",
      "|2000-01-09|         31.004|         15.086|          0.0|0.550392832423845|   151772|\n",
      "|2000-01-10|         31.485|         16.741|          0.0|0.578118385905934|   151772|\n",
      "|2000-01-11|         31.352|         16.577|          0.0| 0.57194784565014|   151772|\n",
      "|2000-01-12|         32.331|          17.92|          0.0|0.515908824934248|   151772|\n",
      "|2000-01-13|         32.555|         21.599|          0.0|0.441207879723881|   151772|\n",
      "|2000-01-14|         33.436|         20.714|          0.0|0.393370570905329|   151772|\n",
      "|2000-01-15|         33.024|         19.683|          0.0|0.420452931588481|   151772|\n",
      "|2000-01-16|         34.143|         21.172|          0.0| 0.39239442680427|   151772|\n",
      "|2000-01-17|         33.353|         17.894|          0.0|0.348336072730764|   151772|\n",
      "|2000-01-18|          33.03|         15.839|          0.0|0.363398190960489|   151772|\n",
      "|2000-01-19|         32.742|         15.953|          0.0|0.397871560983279|   151772|\n",
      "|2000-01-20|         32.969|         15.095|          0.0|0.301888570433364|   151772|\n",
      "+----------+---------------+---------------+-------------+-----------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check data\n",
    "weatherDataDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clubbing all weatherdata files into one single file\n",
    "for file,Id in zip(filenames,stationIDs):\n",
    "    \n",
    "    #load data to dataframe\n",
    "    df = spark.read.load(\n",
    "            path = \"Datasets/2000_2013/files/\"+file,\n",
    "            format=\"csv\", header=True)\n",
    "    \n",
    "    #transforming data to required format\n",
    "    df = ( df.select(\"Date\",F.col(\"Max Temperature\").cast(T.DoubleType()),\n",
    "                     F.col(\"Min Temperature\").cast(T.DoubleType()),\n",
    "                     F.col(\"Precipitation\").cast(T.DoubleType()),\n",
    "                     F.col(\"Relative Humidity\").cast(T.DoubleType()))\n",
    "          .withColumn(\"StationId\",F.lit(Id))\n",
    "          .withColumn(\"Date\",F.to_date(formatDate(F.col(\"Date\")),format=\"yyyy-MM-dd\")))\n",
    "    \n",
    "    #writing data to storage\n",
    "    #using append mode to generate one file\n",
    "    df.write.csv(path = \"Datasets/Output/WeatherData.csv\",\n",
    "                 mode=\"append\",header=True,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
